# -*- coding: utf-8 -*-
"""KNN_USPS_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K6ZETbSNILLCTvz6TO4RvLhpgvVfyijP

**USPS dataset**

---

[Handwitten Digits USPS dataset](http://ieeexplore.ieee.org/document/291440/) has 7291 train and 2007 test images. The images are 16*16 grayscale pixels.
The dataset is given in [hdf5 file format](https://support.hdfgroup.org/HDF5/), the hdf5 file has two groups train and test and each group has two datasets: data and target.

---

References and Citations:

---

[Kaggle Dataset](https://www.kaggle.com/bistaumanga/usps-dataset)  |  [Kaggle - Heart Disease Classification](https://www.kaggle.com/cdabakoglu/heart-disease-classifications-machine-learning)  |  [Github - Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow  03.Classification.ipynb](https://nbviewer.jupyter.org/github/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/03.Classification.ipynb)

---

Textbook - Introduction to Machine Learning with Python ,A Guide 
for Data Scientists by Andreas C. MÃ¼ller and Sarah Guido

---

Demonstration of how to load USPS dataset, visualize and build a K - Neighbors classifier on it that achieves over 90% accuracy over the test set.

---

Assignment 1 - Submitted by Akshay Raina (Id: 1001789877)

**Download, Extract Dataset from .zip and place with Python Notebook together**
"""

import os
import h5py
import sklearn
import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from functools import reduce
from google.colab import files
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report,confusion_matrix

"""**Upload the dataset to Google collaboratory**"""

upload = files.upload()

"""**Initially test if  dataset is uploaded in folder named "content" else adjust folder path accordinly everywhere**"""

print("Files present in current working directory:\n")
print(os.listdir("../content/"))
if os.path.exists(os.path.join(os.getcwd(), '../content/', 'usps.h5')) == True:
  print("\nData Successfully uploaded!")

"""**Function to read USPS data**"""

def hdf5(path, data_key="data",target_key="target", flatten=True ):
  """
      loads data from hdf5: 
      - hdf5 should have 'train' and 'test' groups 
      - each group should have 'data' and 'target' dataset or spcify the key
      - flatten means to flatten images N * (C * H * W) as N * D array
  """
  with h5py.File(path,'r') as hf:
    print("Verify that Dataset is present at path '../content/usps.h5'. Path is",path)
    print("\nhdf5 groups are:" , list(hf.keys()))
    train = hf.get('train')
    X_train = train.get(data_key)[:]
    y_train = train.get(target_key)[:]
    test = hf.get('test')
    X_test = test.get(data_key)[:]
    y_test = test.get(target_key)[:]
    if flatten:
      X_train = X_train.reshape(X_train.shape[0], reduce(lambda a, b: a * b, X_train.shape[1:]))
      X_test = X_test.reshape(X_test.shape[0], reduce(lambda a, b: a * b, X_test.shape[1:]))
  return X_train, y_train, X_test, y_test

X_train, y_train, X_test, y_test = hdf5("../content/usps.h5")
print('\nTrain Group Data dataset shape:',X_train.shape)
print('Train Group Target dataset Shape:',y_train.shape)
print("Train Group Data datset sample: \n",X_train)
print("Train Group Target dataset sample: \n",y_train)
print("\n")
print('Test Group Data dataset shape:',X_test.shape)
print('Test Group Target dataset shape:',y_test.shape)
print("Test Group Data datset sample: ",X_test)
print("Test Group Target dataset sample: ",y_test)

"""**Data Visualization**"""

#Analyzing test group using Visualization
num_samples = 10
num_classes = len(set(y_test))
# or
classes = set(y_test)
num_classes = len(classes)

fig, ax = plt.subplots(num_samples, num_classes, sharex = True, sharey = True, figsize=(num_classes, num_samples))

for label in range(num_classes):
    class_idxs = np.where(y_test == label)
  
    for i, idx in enumerate(np.random.randint(0, class_idxs[0].shape[0], num_samples)):
        ax[i, label].imshow(X_test[class_idxs[0][idx]].reshape([16, 16]), 'gray')
        ax[i, label].set_axis_off()

X, y = X_train[:3500], y_train[:3500]
X.shape, y.shape

some_digit  = X[0]
some_digit_image  = some_digit.reshape(16,16)
plt.imshow(some_digit_image , cmap='binary')
plt.axis('off')
plt.show()

y[0]

"""**Choosing the best K value**

---


The **elbow method** has been used to pick a good K value by concentrating on both accuracy and error rate.

---
1. Choose a K value based on the accuracy rate

"""

# store accuracy rates in array for different values of nearest-neighbours
accuracy_rate = []
K_Values = np.arange(1,250,10)
# We take 250 nearest neighbour values, classifier is used and score is calculated
# by cross-validation and acuracy rates are appended for each values of nearest neighbors
for i in K_Values:
    
    knn = KNeighborsClassifier(n_neighbors=i)
    score=cross_val_score(knn,X_train,y_train,cv=10)
    accuracy_rate.append(score.mean())

plt.figure(figsize=(10,5))
plt.plot(K_Values,accuracy_rate,color='green', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10)
plt.title('Accuracy Rate vs. K Value')
plt.xlabel('K - Value')
plt.ylabel('Accuracy Rate')

"""2. Choose a K value based on the error rate"""

# store error rates in array for different values of nearest-neighbours
error_rate = []
K_Values = np.arange(1,250,10)
# We take 250 nearest neighbour values, classifier is used and score is calculated
# by cross-validation and error rates are appended for each values of nearest neighbors
for i in K_Values:
    
    knn = KNeighborsClassifier(n_neighbors=i)
    score=cross_val_score(knn,X_train,y_train,cv=10)
    error_rate.append(1-score.mean())

plt.figure(figsize=(10,5))
plt.plot(K_Values,error_rate,color='green', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K - Value')
plt.ylabel('Error Rate')

"""**Building a K - Neighbors Classifer**

---


Demonstrating training of K - Neighbors Classifer and computing the accuracy.
"""

# As we seen from above plots, accuracy of 10 nearest neighbors case is good so we select them.

model = KNeighborsClassifier(n_neighbors = 10)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
print("\n# Classification Report for 10 nearest neighbors\n")
print(classification_report(y_test, predictions))

# Confusion Matrix for 10 nearest neighbors
print(confusion_matrix(y_test, predictions))
plt.figure(figsize=(25,13))
plt.subplot(2,3,1)
plt.title("\n# Confusion Matrix for 10 nearest neighbors\n")
sns.heatmap(confusion_matrix(y_test,predictions), annot=True, cmap="Blues", fmt="d", cbar=False, annot_kws={"size":24})
plt.show()

knn_clf = KNeighborsClassifier(n_jobs=-1)
scores = cross_val_score(estimator=knn_clf, X=X_train, y=y_train, scoring='accuracy', cv=5, n_jobs=-1)
scores

grid = {
    'weights': ['uniform', 'distance'],
    'n_neighbors': [2,3,4,5]
}
grid_search_clf = GridSearchCV(estimator=knn_clf,param_grid=grid, scoring='accuracy', cv=5, verbose=3, n_jobs=-1)
grid_search_clf.fit(X=X_train , y=y_train)

grid_search_clf.best_score_

grid_search_clf.best_params_

"""Let's train the model with the best parameters over the whole training set (without cross validation):"""

knn_classifier = KNeighborsClassifier(n_neighbors=2, weights='distance', n_jobs=-1)
knn_classifier.fit(X=X_train , y=y_train)

"""Now we'll evaluate our model over the test set:"""

knn_classifier.score(X=X_test , y=y_test)
acc = knn_classifier.score(X=X_test , y=y_test)
print("Accuracy of K - Neighbors Classifier is",acc,"or",round(acc*100,2),"%.")

"""Saving the model:"""

from joblib import dump
dump(knn_classifier, '../content/knn_USPS_94.37.joblib')